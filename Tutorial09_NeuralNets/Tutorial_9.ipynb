{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 9\n",
    "## Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg\" height=\"200\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "# Fetch the MNIST handwritten digit dataset\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original', data_home=\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_test(model, xtrain, ytrain, xtest, ytest):\n",
    "    model = model.fit(xtrain, ytrain)\n",
    "    output = model.predict(xtest)\n",
    "    print (1.0 * sum([i==j for (i, j) in zip(output, ytest)])) / len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = mnist.data\n",
    "y = mnist.target\n",
    "xy = np.c_[x,y]\n",
    "np.random.shuffle(xy)\n",
    "\n",
    "train_x = xy[0:1000, 0:784]\n",
    "train_y = xy[0:1000, -1]\n",
    "test_x = xy[1000:1100, 0:784]\n",
    "test_y = xy[1000:1100, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_x[1].reshape(28, 28))\n",
    "print train_y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y_probs = np.zeros((1000, 10))\n",
    "for i in range(1000):\n",
    "    train_y_probs[i][int(train_y[i])] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = np.random.randn(784,10)\n",
    "        self.biases = np.random.randn(10)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        out = e_x / e_x.sum()\n",
    "        return out\n",
    "\n",
    "    def fit(self, train_x, train_y):        \n",
    "        for it in range(300):\n",
    "            if it % 100 == 0:\n",
    "                print \"Epoch\", it\n",
    "\n",
    "            for i in range(1000):\n",
    "                # Forward propagation\n",
    "                input_layer = train_x[i]\n",
    "                hidden_layer = np.dot(input_layer, self.weights) + self.biases\n",
    "                output_layer = self.softmax(hidden_layer)\n",
    "\n",
    "                # Back propagation\n",
    "                \n",
    "                grad = output_layer - train_y[i]\n",
    "\n",
    "                w_grad_hid = np.dot(input_layer.reshape(784,1), grad.reshape(1,10))\n",
    "                b_grad_hid = w_grad_hid.sum(axis=0)\n",
    "\n",
    "                self.weights -= w_grad_hid\n",
    "                self.biases -= b_grad_hid\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, test_x):\n",
    "        preds = np.zeros(100)\n",
    "        for i in range(100):\n",
    "            pred = self.softmax(np.dot(test_x[i], self.weights) + self.biases)\n",
    "            preds[i] = np.argmax(pred)\n",
    "        return preds\n",
    "    \n",
    "    def visualize(self):\n",
    "        for i in range(10):\n",
    "            plt.subplot(1, 10, i+1)\n",
    "            plt.axis('off')\n",
    "            plt.set_cmap('gray')\n",
    "            plt.imshow(model.weights[:,i].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "train_and_test(model, train_x, train_y_probs, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Considerations\n",
    "\n",
    "### Deep vs Shalow Neural Networks\n",
    "\n",
    "Decisions regarding the architecture of a neural netowk are some of the most important design decisions involved in using Neural Nets in practice. **Shallow Neural Networks**, with less than 3 layers, can actually aproximate anything, but **Deep Neural Networks** are more flexable, and can aproximate complex relationships on smaller data sets. *However:* the deeper the network, the more susceptible it is to overfitting.\n",
    "\n",
    "### Preventing Overfitting\n",
    "\n",
    "Neural Networks are *highly* suceptable to over-fitting. Given the computational cost of training a nerual network, its important to make sure that they are generalizable to unseen data. This means we need ways to counter over fitting. \n",
    "\n",
    "**Dropout** is the practice of training the neural network on various sets of data and, each time, setting some of the neuron weights to 0. This is analogous to the feature selection process seen in random forests.\n",
    "\n",
    "**Chosing a learning rate** Remember, the neural network uses stochastic gradient descent. Setting $\\eta$ too large will result in the algorithm never converging, setting $\\eta$ too small will make the algorithm take too long. This means that we need to chose $\\eta$, the learning rate. setting it\n",
    "$$\n",
    "\\eta = \\frac k*{1}{n-1}\n",
    "$$\n",
    "will let $\\eta$ decay. Setting it as such will force the algorithm to converge. \n",
    "\n",
    "**Weight Initializations** The weights should be set to random, small values so as to allow each node to contribute to the network. Setting the weights to 0, or in some pattern, will cause the network to be untrainable. Setting the weights too large will have the network's solution be non generalizable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
